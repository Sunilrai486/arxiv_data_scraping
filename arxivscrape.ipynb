{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "def scrape_arxiv(base_url, subject, query, search_url):\n",
    "    # print(search_url)\n",
    "    response = requests.get(search_url)\n",
    "    # print(response)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error: Unable to fetch arXiv data.\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    papers = []\n",
    "\n",
    "    for paper in soup.find_all(\"li\", class_=\"arxiv-result\"):\n",
    "        title = paper.find(\"p\", class_=\"title\").text.strip()\n",
    "        authors = paper.find(\"p\", class_=\"authors\").text.strip()\n",
    "        abstract = paper.find(\"p\", class_=\"abstract\").text.strip()\n",
    "        \n",
    "        # Find all span elements with class=\"tag\"\n",
    "        tag_elements = paper.find_all('span', class_='tag')\n",
    "        tags = [tag.get('data-tooltip') for tag in tag_elements]\n",
    "        \n",
    "        submitted = paper.find(\"p\", class_=\"is-size-7\")\n",
    "        if(submitted == None):\n",
    "            submitted = \"\"\n",
    "        else:\n",
    "            submitted = submitted.text.strip()\n",
    "        \n",
    "        comments = paper.find(\"p\", class_=\"comments\") \n",
    "        if(comments == None):\n",
    "            comments = \"\"\n",
    "        else:\n",
    "            comments = comments.text.strip()\n",
    "        \n",
    "        pdf_url = \"\"\n",
    "        if(not paper.find(\"a\", text=\"pdf\") == None):\n",
    "            pdf_url = paper.find(\"a\", text=\"pdf\")[\"href\"]\n",
    "        arxiv_id = pdf_url.split(\"/\")[-1]\n",
    "\n",
    "        papers.append({\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"abstract\": abstract,\n",
    "            \"tags\": tags,\n",
    "            \"subject\": subject,\n",
    "            \"query\": query,\n",
    "            \"submitted\": submitted,\n",
    "            \"comments\": comments,\n",
    "            \"pdf_url\": base_url + pdf_url,\n",
    "            \"arxiv_id\": arxiv_id\n",
    "        })\n",
    "\n",
    "    return papers\n",
    "\n",
    "def savescrapeddata(subjectquery):\n",
    "    allresults = []\n",
    "    counter = 0\n",
    "    start = 0\n",
    "    subject = subjectquery.split(\":\")[0]\n",
    "    subject_lg = subjectquery.split(\":\")[2]\n",
    "    query = subjectquery.split(\":\")[1]  # Modify this with your desired search query\n",
    "    while(counter < 1):\n",
    "        max_results = 200  # Modify this with the maximum number of results you want\n",
    "        base_url = \"https://arxiv.org\"\n",
    "        query_encoded = urllib.parse.quote_plus(query)\n",
    "        search_url = f\"{base_url}/search/{subject}?query={query_encoded}&searchtype=all&abstracts=show&order=-announced_date_first&size={max_results}\"\n",
    "        # print(search_url)\n",
    "        if(counter > 0):\n",
    "            search_url = search_url + \"&start=\" + str(start)\n",
    "            \n",
    "        results = scrape_arxiv(base_url, subject_lg, query, search_url)\n",
    "        allresults.extend(results)\n",
    "        start += 200\n",
    "        counter += 1\n",
    "    return allresults\n",
    "    # # save results to csv\n",
    "    # import pandas as pd\n",
    "    # df = pd.DataFrame(allresults)\n",
    "    # df.to_csv(f\"arxiv_{subject}_{query}.csv\", index=False)\n",
    "    \n",
    "def searchforallcateogory(searchsubjects):\n",
    "    allresults = []\n",
    "    for subject in searchsubjects:\n",
    "        results = savescrapeddata(subject)\n",
    "        allresults.extend(results)\n",
    "    return allresults\n",
    "         \n",
    "searchsubjects = [\"cs:machine learning:computer science\", \"cs:nlp:computer science\", \"cs:data science:computer science\", \"physics:space science:physics\", \"physics:gravity:physics\", \"physics:quantum physics:physics\", \"math:statistics:mathematics\", \"math:algebra:mathematics\", \"math:probability:mathematics\", \"q-bio:biology:quantitative biology\", \"q-fin:finance:quantative finance\", \"stat:statistics:statistics\", \"eess:electrical engineering:electrical engineering and systems science\", \"econ:economics:economics\"]\n",
    "allresults = searchforallcateogory(searchsubjects) \n",
    "\n",
    "# save results to csv\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(allresults)\n",
    "df.to_csv(f\"arxiv_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests beautifulsoup4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
